2023-08-21 12:30:28,221 INFO [trainer.py:884] (0/2) Training started
2023-08-21 12:30:28,222 INFO [trainer.py:903] (0/2) Device: cuda:0
2023-08-21 12:30:28,222 INFO [trainer.py:904] (0/2) {'best_train_loss': inf, 'best_valid_loss': inf, 'best_train_epoch': -1, 'best_valid_epoch': -1, 'batch_idx_train': 0, 'log_interval': 100, 'reset_interval': 200, 'valid_interval': 20000, 'env_info': {'k2-version': '1.23.4', 'k2-build-type': 'Release', 'k2-with-cuda': True, 'k2-git-sha1': '8802dac96a4eabfc90e967dc8dd967f06b21985e', 'k2-git-date': 'Fri Feb 24 17:56:13 2023', 'lhotse-version': '1.16.0.dev+git.4b443208.dirty', 'torch-version': '1.13.1+cu116', 'torch-cuda-available': True, 'torch-cuda-version': '11.6', 'python-version': '3.1', 'icefall-git-branch': 'e-x', 'icefall-git-sha1': 'b63d8c0-dirty', 'icefall-git-date': 'Mon Aug 21 11:57:05 2023', 'icefall-path': '/home/jovyan/icefall', 'k2-path': '/opt/conda/envs/pytorch/lib/python3.10/site-packages/k2/__init__.py', 'lhotse-path': '/opt/conda/envs/pytorch/lib/python3.10/site-packages/lhotse-1.16.0.dev0+git.4b443208.dirty-py3.10.egg/lhotse/__init__.py', 'hostname': 'nb-b2131-y22fwtuo-0', 'IP address': '10.10.11.212'}, 'world_size': 2, 'master_port': 12354, 'tensorboard': True, 'num_epochs': 20, 'start_epoch': 1, 'start_batch': 0, 'exp_dir': PosixPath('exp/valle'), 'optimizer_name': 'ScaledAdam', 'scheduler_name': 'Eden', 'base_lr': 0.05, 'warmup_steps': 200, 'seed': 42, 'inf_check': False, 'save_every_n': 10000, 'keep_last_k': 20, 'average_period': 0, 'accumulate_grad_steps': 4, 'dtype': 'bfloat16', 'filter_min_duration': 0.5, 'filter_max_duration': 14.0, 'train_stage': 1, 'visualize': False, 'oom_check': False, 'model_name': 'valle', 'decoder_dim': 1024, 'nhead': 16, 'num_decoder_layers': 12, 'scale_factor': 1.0, 'norm_first': True, 'add_prenet': False, 'prefix_mode': 1, 'share_embedding': True, 'prepend_bos': False, 'num_quantizers': 8, 'scaling_xformers': False, 'manifest_dir': PosixPath('data/tokenized'), 'max_duration': 80, 'bucketing_sampler': True, 'num_buckets': 6, 'concatenate_cuts': False, 'duration_factor': 1.0, 'gap': 0.1, 'on_the_fly_feats': False, 'shuffle': True, 'drop_last': False, 'return_cuts': True, 'num_workers': 8, 'enable_spec_aug': False, 'spec_aug_time_warp_factor': 80, 'input_strategy': 'PrecomputedFeatures', 'dataset': 'libritts', 'text_tokens': 'data/tokenized/unique_text_tokens.k2symbols', 'sampling_rate': 24000}
2023-08-21 12:30:28,222 INFO [trainer.py:906] (0/2) About to create model
2023-08-21 12:30:28,222 INFO [trainer.py:884] (1/2) Training started
2023-08-21 12:30:28,222 INFO [trainer.py:903] (1/2) Device: cuda:1
2023-08-21 12:30:28,222 INFO [trainer.py:904] (1/2) {'best_train_loss': inf, 'best_valid_loss': inf, 'best_train_epoch': -1, 'best_valid_epoch': -1, 'batch_idx_train': 0, 'log_interval': 100, 'reset_interval': 200, 'valid_interval': 20000, 'env_info': {'k2-version': '1.23.4', 'k2-build-type': 'Release', 'k2-with-cuda': True, 'k2-git-sha1': '8802dac96a4eabfc90e967dc8dd967f06b21985e', 'k2-git-date': 'Fri Feb 24 17:56:13 2023', 'lhotse-version': '1.16.0.dev+git.4b443208.dirty', 'torch-version': '1.13.1+cu116', 'torch-cuda-available': True, 'torch-cuda-version': '11.6', 'python-version': '3.1', 'icefall-git-branch': 'e-x', 'icefall-git-sha1': 'b63d8c0-dirty', 'icefall-git-date': 'Mon Aug 21 11:57:05 2023', 'icefall-path': '/home/jovyan/icefall', 'k2-path': '/opt/conda/envs/pytorch/lib/python3.10/site-packages/k2/__init__.py', 'lhotse-path': '/opt/conda/envs/pytorch/lib/python3.10/site-packages/lhotse-1.16.0.dev0+git.4b443208.dirty-py3.10.egg/lhotse/__init__.py', 'hostname': 'nb-b2131-y22fwtuo-0', 'IP address': '10.10.11.212'}, 'world_size': 2, 'master_port': 12354, 'tensorboard': True, 'num_epochs': 20, 'start_epoch': 1, 'start_batch': 0, 'exp_dir': PosixPath('exp/valle'), 'optimizer_name': 'ScaledAdam', 'scheduler_name': 'Eden', 'base_lr': 0.05, 'warmup_steps': 200, 'seed': 42, 'inf_check': False, 'save_every_n': 10000, 'keep_last_k': 20, 'average_period': 0, 'accumulate_grad_steps': 4, 'dtype': 'bfloat16', 'filter_min_duration': 0.5, 'filter_max_duration': 14.0, 'train_stage': 1, 'visualize': False, 'oom_check': False, 'model_name': 'valle', 'decoder_dim': 1024, 'nhead': 16, 'num_decoder_layers': 12, 'scale_factor': 1.0, 'norm_first': True, 'add_prenet': False, 'prefix_mode': 1, 'share_embedding': True, 'prepend_bos': False, 'num_quantizers': 8, 'scaling_xformers': False, 'manifest_dir': PosixPath('data/tokenized'), 'max_duration': 80, 'bucketing_sampler': True, 'num_buckets': 6, 'concatenate_cuts': False, 'duration_factor': 1.0, 'gap': 0.1, 'on_the_fly_feats': False, 'shuffle': True, 'drop_last': False, 'return_cuts': True, 'num_workers': 8, 'enable_spec_aug': False, 'spec_aug_time_warp_factor': 80, 'input_strategy': 'PrecomputedFeatures', 'dataset': 'libritts', 'text_tokens': 'data/tokenized/unique_text_tokens.k2symbols', 'sampling_rate': 24000}
2023-08-21 12:30:28,222 INFO [trainer.py:906] (1/2) About to create model
VALLE(
  (ar_text_embedding): TokenEmbedding(
    (dropout): Dropout(p=0.0, inplace=False)
    (word_embeddings): Embedding(512, 1024)
  )
  (nar_text_embedding): TokenEmbedding(
    (dropout): Dropout(p=0.0, inplace=False)
    (word_embeddings): Embedding(512, 1024)
  )
  (ar_audio_embedding): TokenEmbedding(
    (dropout): Dropout(p=0.0, inplace=False)
    (word_embeddings): Embedding(1025, 1024)
  )
  (ar_text_prenet): Identity()
  (ar_audio_prenet): Identity()
  (ar_text_position): SinePositionalEmbedding(
    (dropout): Dropout(p=0.1, inplace=False)
  )
  (ar_audio_position): SinePositionalEmbedding(
    (dropout): Dropout(p=0.1, inplace=False)
  )
  (ar_decoder): TransformerEncoder(
    (layers): ModuleList(
      (0): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)
        )
        (linear1): Linear(in_features=1024, out_features=4096, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=4096, out_features=1024, bias=True)
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.1, inplace=False)
        (norm1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
        (norm2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
      )
      (1): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)
        )
        (linear1): Linear(in_features=1024, out_features=4096, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=4096, out_features=1024, bias=True)
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.1, inplace=False)
        (norm1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
        (norm2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
      )
      (2): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)
        )
        (linear1): Linear(in_features=1024, out_features=4096, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=4096, out_features=1024, bias=True)
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.1, inplace=False)
        (norm1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
        (norm2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
      )
      (3): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)
        )
        (linear1): Linear(in_features=1024, out_features=4096, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=4096, out_features=1024, bias=True)
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.1, inplace=False)
        (norm1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
        (norm2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
      )
      (4): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)
        )
        (linear1): Linear(in_features=1024, out_features=4096, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=4096, out_features=1024, bias=True)
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.1, inplace=False)
        (norm1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
        (norm2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
      )
      (5): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)
        )
        (linear1): Linear(in_features=1024, out_features=4096, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=4096, out_features=1024, bias=True)
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.1, inplace=False)
        (norm1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
        (norm2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
      )
      (6): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)
        )
        (linear1): Linear(in_features=1024, out_features=4096, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=4096, out_features=1024, bias=True)
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.1, inplace=False)
        (norm1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
        (norm2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
      )
      (7): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)
        )
        (linear1): Linear(in_features=1024, out_features=4096, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=4096, out_features=1024, bias=True)
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.1, inplace=False)
        (norm1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
        (norm2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
      )
      (8): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)
        )
        (linear1): Linear(in_features=1024, out_features=4096, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=4096, out_features=1024, bias=True)
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.1, inplace=False)
        (norm1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
        (norm2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
      )
      (9): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)
        )
        (linear1): Linear(in_features=1024, out_features=4096, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=4096, out_features=1024, bias=True)
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.1, inplace=False)
        (norm1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
        (norm2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
      )
      (10): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)
        )
        (linear1): Linear(in_features=1024, out_features=4096, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=4096, out_features=1024, bias=True)
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.1, inplace=False)
        (norm1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
        (norm2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
      )
      (11): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)
        )
        (linear1): Linear(in_features=1024, out_features=4096, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=4096, out_features=1024, bias=True)
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.1, inplace=False)
        (norm1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
        (norm2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
      )
    )
    (norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
  )
  (ar_predict_layer): Linear(in_features=1024, out_features=1025, bias=False)
  (ar_accuracy_metric): MulticlassAccuracy()
  (nar_audio_embeddings): ModuleList(
    (0): TokenEmbedding(
      (dropout): Dropout(p=0.0, inplace=False)
      (word_embeddings): Embedding(1025, 1024)
    )
    (1): TokenEmbedding(
      (dropout): Dropout(p=0.0, inplace=False)
      (word_embeddings): Embedding(1024, 1024)
    )
    (2): TokenEmbedding(
      (dropout): Dropout(p=0.0, inplace=False)
      (word_embeddings): Embedding(1024, 1024)
    )
    (3): TokenEmbedding(
      (dropout): Dropout(p=0.0, inplace=False)
      (word_embeddings): Embedding(1024, 1024)
    )
    (4): TokenEmbedding(
      (dropout): Dropout(p=0.0, inplace=False)
      (word_embeddings): Embedding(1024, 1024)
    )
    (5): TokenEmbedding(
      (dropout): Dropout(p=0.0, inplace=False)
      (word_embeddings): Embedding(1024, 1024)
    )
    (6): TokenEmbedding(
      (dropout): Dropout(p=0.0, inplace=False)
      (word_embeddings): Embedding(1024, 1024)
    )
    (7): TokenEmbedding(
      (dropout): Dropout(p=0.0, inplace=False)
      (word_embeddings): Embedding(1024, 1024)
    )
  )
  (nar_text_prenet): Identity()
  (nar_audio_prenet): Identity()
  (nar_text_position): SinePositionalEmbedding(
    (dropout): Dropout(p=0.0, inplace=False)
  )
  (nar_audio_position): SinePositionalEmbedding(
    (dropout): Dropout(p=0.1, inplace=False)
  )
  (nar_decoder): TransformerEncoder(
    (layers): ModuleList(
      (0): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)
        )
        (linear1): Linear(in_features=1024, out_features=4096, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=4096, out_features=1024, bias=True)
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.1, inplace=False)
        (norm1): AdaptiveLayerNorm(
          (project_layer): Linear(in_features=1024, out_features=2048, bias=True)
          (norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
        )
        (norm2): AdaptiveLayerNorm(
          (project_layer): Linear(in_features=1024, out_features=2048, bias=True)
          (norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
        )
      )
      (1): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)
        )
        (linear1): Linear(in_features=1024, out_features=4096, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=4096, out_features=1024, bias=True)
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.1, inplace=False)
        (norm1): AdaptiveLayerNorm(
          (project_layer): Linear(in_features=1024, out_features=2048, bias=True)
          (norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
        )
        (norm2): AdaptiveLayerNorm(
          (project_layer): Linear(in_features=1024, out_features=2048, bias=True)
          (norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
        )
      )
      (2): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)
        )
        (linear1): Linear(in_features=1024, out_features=4096, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=4096, out_features=1024, bias=True)
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.1, inplace=False)
        (norm1): AdaptiveLayerNorm(
          (project_layer): Linear(in_features=1024, out_features=2048, bias=True)
          (norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
        )
        (norm2): AdaptiveLayerNorm(
          (project_layer): Linear(in_features=1024, out_features=2048, bias=True)
          (norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
        )
      )
      (3): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)
        )
        (linear1): Linear(in_features=1024, out_features=4096, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=4096, out_features=1024, bias=True)
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.1, inplace=False)
        (norm1): AdaptiveLayerNorm(
          (project_layer): Linear(in_features=1024, out_features=2048, bias=True)
          (norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
        )
        (norm2): AdaptiveLayerNorm(
          (project_layer): Linear(in_features=1024, out_features=2048, bias=True)
          (norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
        )
      )
      (4): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)
        )
        (linear1): Linear(in_features=1024, out_features=4096, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=4096, out_features=1024, bias=True)
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.1, inplace=False)
        (norm1): AdaptiveLayerNorm(
          (project_layer): Linear(in_features=1024, out_features=2048, bias=True)
          (norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
        )
        (norm2): AdaptiveLayerNorm(
          (project_layer): Linear(in_features=1024, out_features=2048, bias=True)
          (norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
        )
      )
      (5): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)
        )
        (linear1): Linear(in_features=1024, out_features=4096, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=4096, out_features=1024, bias=True)
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.1, inplace=False)
        (norm1): AdaptiveLayerNorm(
          (project_layer): Linear(in_features=1024, out_features=2048, bias=True)
          (norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
        )
        (norm2): AdaptiveLayerNorm(
          (project_layer): Linear(in_features=1024, out_features=2048, bias=True)
          (norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
        )
      )
      (6): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)
        )
        (linear1): Linear(in_features=1024, out_features=4096, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=4096, out_features=1024, bias=True)
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.1, inplace=False)
        (norm1): AdaptiveLayerNorm(
          (project_layer): Linear(in_features=1024, out_features=2048, bias=True)
          (norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
        )
        (norm2): AdaptiveLayerNorm(
          (project_layer): Linear(in_features=1024, out_features=2048, bias=True)
          (norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
        )
      )
      (7): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)
        )
        (linear1): Linear(in_features=1024, out_features=4096, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=4096, out_features=1024, bias=True)
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.1, inplace=False)
        (norm1): AdaptiveLayerNorm(
          (project_layer): Linear(in_features=1024, out_features=2048, bias=True)
          (norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
        )
        (norm2): AdaptiveLayerNorm(
          (project_layer): Linear(in_features=1024, out_features=2048, bias=True)
          (norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
        )
      )
      (8): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)
        )
        (linear1): Linear(in_features=1024, out_features=4096, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=4096, out_features=1024, bias=True)
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.1, inplace=False)
        (norm1): AdaptiveLayerNorm(
          (project_layer): Linear(in_features=1024, out_features=2048, bias=True)
          (norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
        )
        (norm2): AdaptiveLayerNorm(
          (project_layer): Linear(in_features=1024, out_features=2048, bias=True)
          (norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
        )
      )
      (9): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)
        )
        (linear1): Linear(in_features=1024, out_features=4096, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=4096, out_features=1024, bias=True)
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.1, inplace=False)
        (norm1): AdaptiveLayerNorm(
          (project_layer): Linear(in_features=1024, out_features=2048, bias=True)
          (norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
        )
        (norm2): AdaptiveLayerNorm(
          (project_layer): Linear(in_features=1024, out_features=2048, bias=True)
          (norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
        )
      )
      (10): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)
        )
        (linear1): Linear(in_features=1024, out_features=4096, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=4096, out_features=1024, bias=True)
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.1, inplace=False)
        (norm1): AdaptiveLayerNorm(
          (project_layer): Linear(in_features=1024, out_features=2048, bias=True)
          (norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
        )
        (norm2): AdaptiveLayerNorm(
          (project_layer): Linear(in_features=1024, out_features=2048, bias=True)
          (norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
        )
      )
      (11): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)
        )
        (linear1): Linear(in_features=1024, out_features=4096, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=4096, out_features=1024, bias=True)
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.1, inplace=False)
        (norm1): AdaptiveLayerNorm(
          (project_layer): Linear(in_features=1024, out_features=2048, bias=True)
          (norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
        )
        (norm2): AdaptiveLayerNorm(
          (project_layer): Linear(in_features=1024, out_features=2048, bias=True)
          (norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
        )
      )
    )
    (norm): AdaptiveLayerNorm(
      (project_layer): Linear(in_features=1024, out_features=2048, bias=True)
      (norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
    )
  )
  (nar_predict_layers): ModuleList(
    (0): Linear(in_features=1024, out_features=1024, bias=False)
    (1): Linear(in_features=1024, out_features=1024, bias=False)
    (2): Linear(in_features=1024, out_features=1024, bias=False)
    (3): Linear(in_features=1024, out_features=1024, bias=False)
    (4): Linear(in_features=1024, out_features=1024, bias=False)
    (5): Linear(in_features=1024, out_features=1024, bias=False)
    (6): Linear(in_features=1024, out_features=1024, bias=False)
  )
  (nar_stage_embeddings): ModuleList(
    (0): TokenEmbedding(
      (dropout): Dropout(p=0.0, inplace=False)
      (word_embeddings): Embedding(1, 1024)
    )
    (1): TokenEmbedding(
      (dropout): Dropout(p=0.0, inplace=False)
      (word_embeddings): Embedding(1, 1024)
    )
    (2): TokenEmbedding(
      (dropout): Dropout(p=0.0, inplace=False)
      (word_embeddings): Embedding(1, 1024)
    )
    (3): TokenEmbedding(
      (dropout): Dropout(p=0.0, inplace=False)
      (word_embeddings): Embedding(1, 1024)
    )
    (4): TokenEmbedding(
      (dropout): Dropout(p=0.0, inplace=False)
      (word_embeddings): Embedding(1, 1024)
    )
    (5): TokenEmbedding(
      (dropout): Dropout(p=0.0, inplace=False)
      (word_embeddings): Embedding(1, 1024)
    )
    (6): TokenEmbedding(
      (dropout): Dropout(p=0.0, inplace=False)
      (word_embeddings): Embedding(1, 1024)
    )
  )
  (nar_accuracy_metric): MulticlassAccuracy()
  (lang_embedding): TokenEmbedding(
    (dropout): Dropout(p=0.0, inplace=False)
    (word_embeddings): Embedding(2, 1024)
  )
)2023-08-21 12:30:28,895 INFO [trainer.py:913] (1/2) Number of model parameters: 367388676
VALLE(
  (ar_text_embedding): TokenEmbedding(
    (dropout): Dropout(p=0.0, inplace=False)
    (word_embeddings): Embedding(512, 1024)
  )
  (nar_text_embedding): TokenEmbedding(
    (dropout): Dropout(p=0.0, inplace=False)
    (word_embeddings): Embedding(512, 1024)
  )
  (ar_audio_embedding): TokenEmbedding(
    (dropout): Dropout(p=0.0, inplace=False)
    (word_embeddings): Embedding(1025, 1024)
  )
  (ar_text_prenet): Identity()
  (ar_audio_prenet): Identity()
  (ar_text_position): SinePositionalEmbedding(
    (dropout): Dropout(p=0.1, inplace=False)
  )
  (ar_audio_position): SinePositionalEmbedding(
    (dropout): Dropout(p=0.1, inplace=False)
  )
  (ar_decoder): TransformerEncoder(
    (layers): ModuleList(
      (0): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)
        )
        (linear1): Linear(in_features=1024, out_features=4096, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=4096, out_features=1024, bias=True)
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.1, inplace=False)
        (norm1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
        (norm2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
      )
      (1): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)
        )
        (linear1): Linear(in_features=1024, out_features=4096, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=4096, out_features=1024, bias=True)
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.1, inplace=False)
        (norm1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
        (norm2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
      )
      (2): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)
        )
        (linear1): Linear(in_features=1024, out_features=4096, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=4096, out_features=1024, bias=True)
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.1, inplace=False)
        (norm1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
        (norm2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
      )
      (3): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)
        )
        (linear1): Linear(in_features=1024, out_features=4096, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=4096, out_features=1024, bias=True)
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.1, inplace=False)
        (norm1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
        (norm2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
      )
      (4): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)
        )
        (linear1): Linear(in_features=1024, out_features=4096, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=4096, out_features=1024, bias=True)
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.1, inplace=False)
        (norm1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
        (norm2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
      )
      (5): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)
        )
        (linear1): Linear(in_features=1024, out_features=4096, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=4096, out_features=1024, bias=True)
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.1, inplace=False)
        (norm1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
        (norm2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
      )
      (6): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)
        )
        (linear1): Linear(in_features=1024, out_features=4096, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=4096, out_features=1024, bias=True)
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.1, inplace=False)
        (norm1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
        (norm2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
      )
      (7): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)
        )
        (linear1): Linear(in_features=1024, out_features=4096, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=4096, out_features=1024, bias=True)
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.1, inplace=False)
        (norm1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
        (norm2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
      )
      (8): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)
        )
        (linear1): Linear(in_features=1024, out_features=4096, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=4096, out_features=1024, bias=True)
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.1, inplace=False)
        (norm1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
        (norm2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
      )
      (9): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)
        )
        (linear1): Linear(in_features=1024, out_features=4096, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=4096, out_features=1024, bias=True)
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.1, inplace=False)
        (norm1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
        (norm2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
      )
      (10): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)
        )
        (linear1): Linear(in_features=1024, out_features=4096, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=4096, out_features=1024, bias=True)
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.1, inplace=False)
        (norm1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
        (norm2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
      )
      (11): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)
        )
        (linear1): Linear(in_features=1024, out_features=4096, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=4096, out_features=1024, bias=True)
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.1, inplace=False)
        (norm1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
        (norm2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
      )
    )
    (norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
  )
  (ar_predict_layer): Linear(in_features=1024, out_features=1025, bias=False)
  (ar_accuracy_metric): MulticlassAccuracy()
  (nar_audio_embeddings): ModuleList(
    (0): TokenEmbedding(
      (dropout): Dropout(p=0.0, inplace=False)
      (word_embeddings): Embedding(1025, 1024)
    )
    (1): TokenEmbedding(
      (dropout): Dropout(p=0.0, inplace=False)
      (word_embeddings): Embedding(1024, 1024)
    )
    (2): TokenEmbedding(
      (dropout): Dropout(p=0.0, inplace=False)
      (word_embeddings): Embedding(1024, 1024)
    )
    (3): TokenEmbedding(
      (dropout): Dropout(p=0.0, inplace=False)
      (word_embeddings): Embedding(1024, 1024)
    )
    (4): TokenEmbedding(
      (dropout): Dropout(p=0.0, inplace=False)
      (word_embeddings): Embedding(1024, 1024)
    )
    (5): TokenEmbedding(
      (dropout): Dropout(p=0.0, inplace=False)
      (word_embeddings): Embedding(1024, 1024)
    )
    (6): TokenEmbedding(
      (dropout): Dropout(p=0.0, inplace=False)
      (word_embeddings): Embedding(1024, 1024)
    )
    (7): TokenEmbedding(
      (dropout): Dropout(p=0.0, inplace=False)
      (word_embeddings): Embedding(1024, 1024)
    )
  )
  (nar_text_prenet): Identity()
  (nar_audio_prenet): Identity()
  (nar_text_position): SinePositionalEmbedding(
    (dropout): Dropout(p=0.0, inplace=False)
  )
  (nar_audio_position): SinePositionalEmbedding(
    (dropout): Dropout(p=0.1, inplace=False)
  )
  (nar_decoder): TransformerEncoder(
    (layers): ModuleList(
      (0): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)
        )
        (linear1): Linear(in_features=1024, out_features=4096, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=4096, out_features=1024, bias=True)
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.1, inplace=False)
        (norm1): AdaptiveLayerNorm(
          (project_layer): Linear(in_features=1024, out_features=2048, bias=True)
          (norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
        )
        (norm2): AdaptiveLayerNorm(
          (project_layer): Linear(in_features=1024, out_features=2048, bias=True)
          (norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
        )
      )
      (1): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)
        )
        (linear1): Linear(in_features=1024, out_features=4096, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=4096, out_features=1024, bias=True)
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.1, inplace=False)
        (norm1): AdaptiveLayerNorm(
          (project_layer): Linear(in_features=1024, out_features=2048, bias=True)
          (norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
        )
        (norm2): AdaptiveLayerNorm(
          (project_layer): Linear(in_features=1024, out_features=2048, bias=True)
          (norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
        )
      )
      (2): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)
        )
        (linear1): Linear(in_features=1024, out_features=4096, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=4096, out_features=1024, bias=True)
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.1, inplace=False)
        (norm1): AdaptiveLayerNorm(
          (project_layer): Linear(in_features=1024, out_features=2048, bias=True)
          (norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
        )
        (norm2): AdaptiveLayerNorm(
          (project_layer): Linear(in_features=1024, out_features=2048, bias=True)
          (norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
        )
      )
      (3): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)
        )
        (linear1): Linear(in_features=1024, out_features=4096, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=4096, out_features=1024, bias=True)
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.1, inplace=False)
        (norm1): AdaptiveLayerNorm(
          (project_layer): Linear(in_features=1024, out_features=2048, bias=True)
          (norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
        )
        (norm2): AdaptiveLayerNorm(
          (project_layer): Linear(in_features=1024, out_features=2048, bias=True)
          (norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
        )
      )
      (4): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)
        )
        (linear1): Linear(in_features=1024, out_features=4096, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=4096, out_features=1024, bias=True)
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.1, inplace=False)
        (norm1): AdaptiveLayerNorm(
          (project_layer): Linear(in_features=1024, out_features=2048, bias=True)
          (norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
        )
        (norm2): AdaptiveLayerNorm(
          (project_layer): Linear(in_features=1024, out_features=2048, bias=True)
          (norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
        )
      )
      (5): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)
        )
        (linear1): Linear(in_features=1024, out_features=4096, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=4096, out_features=1024, bias=True)
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.1, inplace=False)
        (norm1): AdaptiveLayerNorm(
          (project_layer): Linear(in_features=1024, out_features=2048, bias=True)
          (norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
        )
        (norm2): AdaptiveLayerNorm(
          (project_layer): Linear(in_features=1024, out_features=2048, bias=True)
          (norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
        )
      )
      (6): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)
        )
        (linear1): Linear(in_features=1024, out_features=4096, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=4096, out_features=1024, bias=True)
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.1, inplace=False)
        (norm1): AdaptiveLayerNorm(
          (project_layer): Linear(in_features=1024, out_features=2048, bias=True)
          (norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
        )
        (norm2): AdaptiveLayerNorm(
          (project_layer): Linear(in_features=1024, out_features=2048, bias=True)
          (norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
        )
      )
      (7): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)
        )
        (linear1): Linear(in_features=1024, out_features=4096, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=4096, out_features=1024, bias=True)
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.1, inplace=False)
        (norm1): AdaptiveLayerNorm(
          (project_layer): Linear(in_features=1024, out_features=2048, bias=True)
          (norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
        )
        (norm2): AdaptiveLayerNorm(
          (project_layer): Linear(in_features=1024, out_features=2048, bias=True)
          (norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
        )
      )
      (8): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)
        )
        (linear1): Linear(in_features=1024, out_features=4096, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=4096, out_features=1024, bias=True)
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.1, inplace=False)
        (norm1): AdaptiveLayerNorm(
          (project_layer): Linear(in_features=1024, out_features=2048, bias=True)
          (norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
        )
        (norm2): AdaptiveLayerNorm(
          (project_layer): Linear(in_features=1024, out_features=2048, bias=True)
          (norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
        )
      )
      (9): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)
        )
        (linear1): Linear(in_features=1024, out_features=4096, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=4096, out_features=1024, bias=True)
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.1, inplace=False)
        (norm1): AdaptiveLayerNorm(
          (project_layer): Linear(in_features=1024, out_features=2048, bias=True)
          (norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
        )
        (norm2): AdaptiveLayerNorm(
          (project_layer): Linear(in_features=1024, out_features=2048, bias=True)
          (norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
        )
      )
      (10): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)
        )
        (linear1): Linear(in_features=1024, out_features=4096, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=4096, out_features=1024, bias=True)
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.1, inplace=False)
        (norm1): AdaptiveLayerNorm(
          (project_layer): Linear(in_features=1024, out_features=2048, bias=True)
          (norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
        )
        (norm2): AdaptiveLayerNorm(
          (project_layer): Linear(in_features=1024, out_features=2048, bias=True)
          (norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
        )
      )
      (11): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)
        )
        (linear1): Linear(in_features=1024, out_features=4096, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=4096, out_features=1024, bias=True)
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.1, inplace=False)
        (norm1): AdaptiveLayerNorm(
          (project_layer): Linear(in_features=1024, out_features=2048, bias=True)
          (norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
        )
        (norm2): AdaptiveLayerNorm(
          (project_layer): Linear(in_features=1024, out_features=2048, bias=True)
          (norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
        )
      )
    )
    (norm): AdaptiveLayerNorm(
      (project_layer): Linear(in_features=1024, out_features=2048, bias=True)
      (norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
    )
  )
  (nar_predict_layers): ModuleList(
    (0): Linear(in_features=1024, out_features=1024, bias=False)
    (1): Linear(in_features=1024, out_features=1024, bias=False)
    (2): Linear(in_features=1024, out_features=1024, bias=False)
    (3): Linear(in_features=1024, out_features=1024, bias=False)
    (4): Linear(in_features=1024, out_features=1024, bias=False)
    (5): Linear(in_features=1024, out_features=1024, bias=False)
    (6): Linear(in_features=1024, out_features=1024, bias=False)
  )
  (nar_stage_embeddings): ModuleList(
    (0): TokenEmbedding(
      (dropout): Dropout(p=0.0, inplace=False)
      (word_embeddings): Embedding(1, 1024)
    )
    (1): TokenEmbedding(
      (dropout): Dropout(p=0.0, inplace=False)
      (word_embeddings): Embedding(1, 1024)
    )
    (2): TokenEmbedding(
      (dropout): Dropout(p=0.0, inplace=False)
      (word_embeddings): Embedding(1, 1024)
    )
    (3): TokenEmbedding(
      (dropout): Dropout(p=0.0, inplace=False)
      (word_embeddings): Embedding(1, 1024)
    )
    (4): TokenEmbedding(
      (dropout): Dropout(p=0.0, inplace=False)
      (word_embeddings): Embedding(1, 1024)
    )
    (5): TokenEmbedding(
      (dropout): Dropout(p=0.0, inplace=False)
      (word_embeddings): Embedding(1, 1024)
    )
    (6): TokenEmbedding(
      (dropout): Dropout(p=0.0, inplace=False)
      (word_embeddings): Embedding(1, 1024)
    )
  )
  (nar_accuracy_metric): MulticlassAccuracy()
  (lang_embedding): TokenEmbedding(
    (dropout): Dropout(p=0.0, inplace=False)
    (word_embeddings): Embedding(2, 1024)
  )
)2023-08-21 12:30:28,899 INFO [trainer.py:913] (0/2) Number of model parameters: 367388676
2023-08-21 12:30:30,470 INFO [trainer.py:928] (1/2) Using DDP
2023-08-21 12:30:30,489 INFO [trainer.py:928] (0/2) Using DDP
2023-08-21 12:30:30,731 INFO [datamodule.py:417] (1/2) About to get train cuts
2023-08-21 12:30:30,732 INFO [datamodule.py:417] (0/2) About to get train cuts
